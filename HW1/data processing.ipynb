{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8528fb80-5a5f-46f7-b5b5-70d26ebe029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bce71d11-6dcc-4926-9700-29da883a393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = pd.read_csv(\"train.csv\")\n",
    "train_ds = train_ds.dropna(axis=0, how=\"any\") \n",
    "test_ds = pd.read_csv(\"test.csv\")\n",
    "test_ds = test_ds.dropna(axis=0, how=\"any\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "395fbaaf-f8b4-4cfb-a099-72d683d2cc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27383</td>\n",
       "      <td>i feel awful about it too because it s my job ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110083</td>\n",
       "      <td>im alone i feel awful</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140764</td>\n",
       "      <td>ive probably mentioned this before but i reall...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100071</td>\n",
       "      <td>i was feeling a little low few days back</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2837</td>\n",
       "      <td>i beleive that i am much more sensitive to oth...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text emotions\n",
       "0   27383  i feel awful about it too because it s my job ...  sadness\n",
       "1  110083                              im alone i feel awful  sadness\n",
       "2  140764  ive probably mentioned this before but i reall...      joy\n",
       "3  100071           i was feeling a little low few days back  sadness\n",
       "4    2837  i beleive that i am much more sensitive to oth...     love"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf8641ce-09c5-4cf9-9b0b-174956fb100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = train_ds[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "985fed91-bd65-4994-bb49-79222b9e606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = test_ds[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdff330c-7690-44bb-9811-ddde309051ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_uniquewords = set(train_words.str.lower().str.findall(\"\\w+\").sum())\n",
    "test_uniquewords = set(test_words.str.lower().str.findall(\"\\w+\").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "856df623-89f4-4194-ad71-9017d20d67da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'continued',\n",
       " 'potential',\n",
       " 'wellbeing',\n",
       " 'medication',\n",
       " 'married',\n",
       " 'recipient',\n",
       " 'remove',\n",
       " 'sustains',\n",
       " 'galleries',\n",
       " 'jacob',\n",
       " 'skating',\n",
       " 'term',\n",
       " 'phd',\n",
       " 'calls',\n",
       " 'award',\n",
       " 'someday',\n",
       " 'laughs',\n",
       " 'yellow',\n",
       " 'absurd',\n",
       " 'observer',\n",
       " 'disorders',\n",
       " 'bennett',\n",
       " 'stigmatized',\n",
       " 'sceptical',\n",
       " 'passage',\n",
       " 'recipricated',\n",
       " 'mountie',\n",
       " 'streets',\n",
       " 'depressing',\n",
       " 'nearly',\n",
       " 'insercurity',\n",
       " 'edgar',\n",
       " 'alt',\n",
       " 'approval',\n",
       " 'tips',\n",
       " 'testosterone',\n",
       " 'applications',\n",
       " 'illustrator',\n",
       " 'fresh',\n",
       " 'loose',\n",
       " 'exclude',\n",
       " 'spice',\n",
       " 'bitterness',\n",
       " 'services',\n",
       " 'clearboth',\n",
       " 'allow',\n",
       " 'prophetically',\n",
       " 'randomly',\n",
       " 'grumbled',\n",
       " 'career',\n",
       " 'prolific',\n",
       " 'furry',\n",
       " 'wrench',\n",
       " 'encourage',\n",
       " 'insincere',\n",
       " 'drunk',\n",
       " 'december',\n",
       " 'discuss',\n",
       " 'moves',\n",
       " 'prayer',\n",
       " 'fascinating',\n",
       " 'acquisition',\n",
       " 'exemptions',\n",
       " 'readers',\n",
       " 'niya',\n",
       " 'filed',\n",
       " 'pleasantly',\n",
       " 'reconsider',\n",
       " 'proverbsverse',\n",
       " 'confidence',\n",
       " 'brat',\n",
       " 'remarks',\n",
       " 'indecisive',\n",
       " 'kinabuhi',\n",
       " 'ear',\n",
       " 'completism',\n",
       " 'edward',\n",
       " 'innermost',\n",
       " 'pew',\n",
       " 'track',\n",
       " 'landscape',\n",
       " 'sugar',\n",
       " 'actor',\n",
       " 'fulfilling',\n",
       " 'treatment',\n",
       " 'ben',\n",
       " 'gu',\n",
       " 'rolling',\n",
       " 'authoritative',\n",
       " 'fundraiser',\n",
       " 'thesis',\n",
       " 'wyatt',\n",
       " 'values',\n",
       " 'inhibited',\n",
       " 'independent',\n",
       " 'savage',\n",
       " 'ban',\n",
       " 'explode',\n",
       " 'africa',\n",
       " 'dreaming',\n",
       " 'filing',\n",
       " 'tick',\n",
       " 'despair',\n",
       " 'irate',\n",
       " 'inflation',\n",
       " 'crew',\n",
       " 'influencing',\n",
       " 'crazed',\n",
       " 'remains',\n",
       " 'mockery',\n",
       " 'assholes',\n",
       " 'padding',\n",
       " 'consistent',\n",
       " 'upto',\n",
       " 'gyn',\n",
       " 'eastern',\n",
       " 'midterm',\n",
       " 'shae',\n",
       " 'revolving',\n",
       " 'churches',\n",
       " 'fiction',\n",
       " 'nation',\n",
       " 'familys',\n",
       " 'grief',\n",
       " 'wander',\n",
       " 'admitted',\n",
       " 'turkey',\n",
       " 'discusted',\n",
       " 'bullet',\n",
       " 'smaug',\n",
       " 'matt',\n",
       " 'turmoil',\n",
       " 'mascara',\n",
       " 'roots',\n",
       " 'compunction',\n",
       " 'purchase',\n",
       " 'crowd',\n",
       " 'providing',\n",
       " 'victor',\n",
       " 'shmuck',\n",
       " 'cuter',\n",
       " 'sacred',\n",
       " 'invisible',\n",
       " 'bikini',\n",
       " 'someoen',\n",
       " 'ugh',\n",
       " 'inspiring',\n",
       " 'confronted',\n",
       " 'private',\n",
       " 'evaporates',\n",
       " 'realli',\n",
       " 'harness',\n",
       " 'complaints',\n",
       " 'cheer',\n",
       " 'toms',\n",
       " 'haha',\n",
       " 'uw',\n",
       " 'educational',\n",
       " 'governor',\n",
       " 'examine',\n",
       " 'stephen',\n",
       " 'exciting',\n",
       " 'uploads',\n",
       " 'certificate',\n",
       " 'supplements',\n",
       " 'dile',\n",
       " 'lovelies',\n",
       " 'whiterun',\n",
       " 'conformist',\n",
       " 'speech',\n",
       " 'religions',\n",
       " 'adjust',\n",
       " 'counseling',\n",
       " 'dad',\n",
       " 'anonymous',\n",
       " 'jumping',\n",
       " 'return',\n",
       " 'plank',\n",
       " 'secondary',\n",
       " 'chuckle',\n",
       " 'kinship',\n",
       " 'syncaines',\n",
       " 'diagnosed',\n",
       " 'father',\n",
       " 'prayers',\n",
       " 'larger',\n",
       " 'kidney',\n",
       " 'loathing',\n",
       " 'civil',\n",
       " 'asses',\n",
       " 'robbers',\n",
       " 'divorces',\n",
       " 'developed',\n",
       " 'babies',\n",
       " 'heal',\n",
       " 'sweden',\n",
       " 'filling',\n",
       " 'slave',\n",
       " 'coffee',\n",
       " 'resource',\n",
       " 'moniker',\n",
       " 'reflected',\n",
       " 'stoked',\n",
       " 'products',\n",
       " 'blackberriesandsparkles',\n",
       " 'sweating',\n",
       " 'basketball',\n",
       " 'slowly',\n",
       " 'continuing',\n",
       " 'obnoxious',\n",
       " 'carried',\n",
       " 'based',\n",
       " 'lady',\n",
       " 'dissonance',\n",
       " 'yuletide',\n",
       " 'washout',\n",
       " 'hundreds',\n",
       " 'rides',\n",
       " 'announcements',\n",
       " 'inhaled',\n",
       " 'pseudo',\n",
       " 'logically',\n",
       " 'empowered',\n",
       " 'ears',\n",
       " 'master',\n",
       " 'bathroom',\n",
       " 'k',\n",
       " 'paparazi',\n",
       " 'righteous',\n",
       " 'changing',\n",
       " 'exact',\n",
       " 'chick',\n",
       " 'jealousy',\n",
       " 'across',\n",
       " 'goodnight',\n",
       " 'hopes',\n",
       " 'illness',\n",
       " 'barcelona',\n",
       " 'smells',\n",
       " 'gun',\n",
       " 'paranoia',\n",
       " 'apenchantforpaper',\n",
       " 'pervish',\n",
       " 'grounds',\n",
       " 'admiration',\n",
       " 'hanky',\n",
       " 'burden',\n",
       " 'purely',\n",
       " 'essay',\n",
       " 'certainty',\n",
       " 'fifth',\n",
       " 'period',\n",
       " 'carton',\n",
       " 'infuriated',\n",
       " 'media',\n",
       " 'forced',\n",
       " 'disaronno',\n",
       " 'gelly',\n",
       " 'charmed',\n",
       " 'ridden',\n",
       " 'represent',\n",
       " 'doctor',\n",
       " 'caused',\n",
       " 'brazen',\n",
       " 'atletico',\n",
       " 'endorphin',\n",
       " 'renaissance',\n",
       " 'urdu',\n",
       " 'traveling',\n",
       " 'yolk',\n",
       " 'whoever',\n",
       " 'superior',\n",
       " 'dichotomy',\n",
       " 'accomplishment',\n",
       " 'eagerness',\n",
       " 'blond',\n",
       " 'adults',\n",
       " 'examinations',\n",
       " 'companies',\n",
       " 'perry',\n",
       " 'freakin',\n",
       " 'focus',\n",
       " 'names',\n",
       " 'september',\n",
       " 'eliminated',\n",
       " 'stupor',\n",
       " 'mudangat',\n",
       " 'vigorous',\n",
       " 'sohai',\n",
       " 'countenances',\n",
       " 'chums',\n",
       " 'sleeps',\n",
       " 'classrooms',\n",
       " 'girly',\n",
       " 'france',\n",
       " 'ic',\n",
       " 'mode',\n",
       " 'unworthy',\n",
       " 'childrens',\n",
       " 'wil',\n",
       " 'coverage',\n",
       " 'placed',\n",
       " 'bits',\n",
       " 'doula',\n",
       " 'design',\n",
       " 'despaired',\n",
       " 'thinks',\n",
       " 'square',\n",
       " 'laden',\n",
       " 'pagetitle',\n",
       " 'sniffled',\n",
       " 'sweat',\n",
       " 'entertaining',\n",
       " 'promoted',\n",
       " 'colleagues',\n",
       " 'display',\n",
       " 'unnoticed',\n",
       " 'subtle',\n",
       " 'cultures',\n",
       " 'crituque',\n",
       " 'mimic',\n",
       " 'backpack',\n",
       " 'similar',\n",
       " 'waist',\n",
       " 'joke',\n",
       " 'recommendations',\n",
       " 'swear',\n",
       " 'crystal',\n",
       " 'irinas',\n",
       " 'triumphant',\n",
       " 'chickens',\n",
       " 'eagerly',\n",
       " 'reducing',\n",
       " 'security',\n",
       " 'kasakit',\n",
       " 'shatter',\n",
       " 'burtreynoldsismyfodder',\n",
       " 'cups',\n",
       " 'firearms',\n",
       " 'roar',\n",
       " 'happiness',\n",
       " 'yesterdays',\n",
       " 'dependent',\n",
       " 'jie',\n",
       " 'generation',\n",
       " 'influence',\n",
       " 'universe',\n",
       " 'succinctly',\n",
       " 'sigh',\n",
       " 'encountered',\n",
       " 'puffed',\n",
       " 'armed',\n",
       " 'trashy',\n",
       " 'touching',\n",
       " 'google',\n",
       " 'powers',\n",
       " 'freeexperiancredit',\n",
       " 'earn',\n",
       " 'abundance',\n",
       " 'projection',\n",
       " 'europe',\n",
       " 'targets',\n",
       " 'ethnicity',\n",
       " 'hulu',\n",
       " 'ordinary',\n",
       " 'shock',\n",
       " 'youth',\n",
       " 'trash',\n",
       " 'clue',\n",
       " 'sterotype',\n",
       " 'advance',\n",
       " 'gas',\n",
       " 'polite',\n",
       " 'guild',\n",
       " 'yin',\n",
       " 'shares',\n",
       " 'resources',\n",
       " 'restore',\n",
       " 'ups',\n",
       " 'trampoline',\n",
       " 'settling',\n",
       " 'understandably',\n",
       " 'miracles',\n",
       " 'muscles',\n",
       " 'task',\n",
       " 'fianc',\n",
       " 'resist',\n",
       " 'evans',\n",
       " 'spouting',\n",
       " 'major',\n",
       " 'sexual',\n",
       " 'pinpoint',\n",
       " 'alexa',\n",
       " 'conflicted',\n",
       " 'contact',\n",
       " 'fellows',\n",
       " 'intimate',\n",
       " 'research',\n",
       " 'lightly',\n",
       " 'lovemaking',\n",
       " 'def',\n",
       " 'bedroom',\n",
       " 'spendinag',\n",
       " 'questioning',\n",
       " 'triggered',\n",
       " 'pizza',\n",
       " 'frightened',\n",
       " 'appreicated',\n",
       " 'literature',\n",
       " 'dedicate',\n",
       " 'escaping',\n",
       " 'currently',\n",
       " 'anthony',\n",
       " 'virgin',\n",
       " 'shubhi',\n",
       " 'barrier',\n",
       " 'banana',\n",
       " 'pregnancy',\n",
       " 'salty',\n",
       " 'ahhhh',\n",
       " 'groups',\n",
       " 'deliciously',\n",
       " 'lab',\n",
       " 'brutality',\n",
       " 'outlast',\n",
       " 'tantric',\n",
       " 'steve',\n",
       " 'pakistan',\n",
       " 'inject',\n",
       " 'needing',\n",
       " 'retarded',\n",
       " 'order',\n",
       " 'washes',\n",
       " 'royals',\n",
       " 'served',\n",
       " 'gross',\n",
       " 'paint',\n",
       " 'apart',\n",
       " 'mumbai',\n",
       " 'rain',\n",
       " 'react',\n",
       " 'homes',\n",
       " 'linguistics',\n",
       " 'unusually',\n",
       " 'kitchen',\n",
       " 'untied',\n",
       " 'antonio',\n",
       " 'machine',\n",
       " 'suffered',\n",
       " 'factor',\n",
       " 'unwind',\n",
       " 'capturing',\n",
       " 'resiliency',\n",
       " 'cspan',\n",
       " 'patience',\n",
       " 'eaten',\n",
       " 'arashi',\n",
       " 'functionally',\n",
       " 'films',\n",
       " 'ing',\n",
       " 'sweats',\n",
       " 'claiming',\n",
       " 'screech',\n",
       " 'newsfeed',\n",
       " 'prideful',\n",
       " 'sensation',\n",
       " 'adrian',\n",
       " 'suddenly',\n",
       " 'develop',\n",
       " 'cocoa',\n",
       " 'ob',\n",
       " 'evaporate',\n",
       " 'employee',\n",
       " 'rip',\n",
       " 'road',\n",
       " 'prior',\n",
       " 'hilton',\n",
       " 'carbonara',\n",
       " 'barsoom',\n",
       " 'lifes',\n",
       " 'bookstores',\n",
       " 'simultaneously',\n",
       " 'ordered',\n",
       " 'produced',\n",
       " 'hates',\n",
       " 'lfe',\n",
       " 'seastar',\n",
       " 'applied',\n",
       " 'sam',\n",
       " 'buddys',\n",
       " 'plot',\n",
       " 'teenagers',\n",
       " 'whatever',\n",
       " 'contrast',\n",
       " 'note',\n",
       " 'families',\n",
       " 'emmas',\n",
       " 'tornado',\n",
       " 'friendships',\n",
       " 'dragging',\n",
       " 'above',\n",
       " 'pursued',\n",
       " 'restaurants',\n",
       " 'motherhood',\n",
       " 'utilize',\n",
       " 'disadvantaged',\n",
       " 'simplification',\n",
       " 'humor',\n",
       " 'clingy',\n",
       " 'nephews',\n",
       " 'spaghetti',\n",
       " 'dake',\n",
       " 'colour',\n",
       " 'splendid',\n",
       " 'cosmopolitan',\n",
       " 'bring',\n",
       " 'adams',\n",
       " 'ant',\n",
       " 'location',\n",
       " 'hurst',\n",
       " 'luke',\n",
       " 'relevent',\n",
       " 'darker',\n",
       " 'benefit',\n",
       " 'birth',\n",
       " 'bipolars',\n",
       " 'portrayal',\n",
       " 'waffling',\n",
       " 'flash',\n",
       " 'neuroblastoma',\n",
       " 'plug',\n",
       " 'slug',\n",
       " 'belonging',\n",
       " 'foot',\n",
       " 'nonsense',\n",
       " 'downstairs',\n",
       " 'abandoned',\n",
       " 'tt',\n",
       " 'irrational',\n",
       " 'unapologetically',\n",
       " 'twenty',\n",
       " 'intimidating',\n",
       " 'studied',\n",
       " 'posh',\n",
       " 'taper',\n",
       " 'yourself',\n",
       " 'fifteen',\n",
       " 'bolder',\n",
       " 'burger',\n",
       " 'realitybite',\n",
       " 'gang',\n",
       " 'oriented',\n",
       " 'perception',\n",
       " 'mystery',\n",
       " 'interaction',\n",
       " 'pwp',\n",
       " 'quilt',\n",
       " 'initial',\n",
       " 'existential',\n",
       " 'turns',\n",
       " 'lol',\n",
       " 'dekiru',\n",
       " 'atmosphere',\n",
       " 'study',\n",
       " 'icon',\n",
       " 'suicidal',\n",
       " 'cx',\n",
       " 'smackeroos',\n",
       " 'privacy',\n",
       " 'kanunay',\n",
       " 'temperament',\n",
       " 'jin',\n",
       " 'blogging',\n",
       " 'faye',\n",
       " 'addressed',\n",
       " 'immersed',\n",
       " 'gmail',\n",
       " 'conservatorie',\n",
       " 'essentially',\n",
       " 'expensive',\n",
       " 'compared',\n",
       " 'elegant',\n",
       " 'unruly',\n",
       " 'unrealistic',\n",
       " 'particles',\n",
       " 'newborn',\n",
       " 'nominated',\n",
       " 'leap',\n",
       " 'overlooked',\n",
       " 'noticed',\n",
       " 'rear',\n",
       " 'unduly',\n",
       " 'evilness',\n",
       " 'posting',\n",
       " 'quote',\n",
       " 'pmsing',\n",
       " 'principles',\n",
       " 'studio',\n",
       " 'reaction',\n",
       " 'dillan',\n",
       " 'sees',\n",
       " 'yes',\n",
       " 'hundred',\n",
       " 'photoshop',\n",
       " 'bloodhound',\n",
       " 'primitive',\n",
       " 'account',\n",
       " 'rock',\n",
       " 'woken',\n",
       " 'grab',\n",
       " 'dragged',\n",
       " 'focussing',\n",
       " 'preggo',\n",
       " 'finalized',\n",
       " 'freely',\n",
       " 'park',\n",
       " 'jreyez',\n",
       " 'mythologies',\n",
       " 'flags',\n",
       " 'compromise',\n",
       " 'crime',\n",
       " 'medusa',\n",
       " 'menopause',\n",
       " 'lace',\n",
       " 'bites',\n",
       " 'intern',\n",
       " 'land',\n",
       " 'curve',\n",
       " 'culottes',\n",
       " 'womanhood',\n",
       " 'albeit',\n",
       " 'fandom',\n",
       " 'whirlpool',\n",
       " 'spoiling',\n",
       " 'livy',\n",
       " 'healers',\n",
       " 'registry',\n",
       " 'genuine',\n",
       " 'disheartened',\n",
       " 'arguing',\n",
       " 'ard',\n",
       " 'rarely',\n",
       " 'thousands',\n",
       " 'updates',\n",
       " 'browsing',\n",
       " 'quoting',\n",
       " 'facts',\n",
       " 'affordable',\n",
       " 'gel',\n",
       " 'flowing',\n",
       " 'aspect',\n",
       " 'gt',\n",
       " 'picking',\n",
       " 'nuclear',\n",
       " 'street',\n",
       " 'poems',\n",
       " 'campness',\n",
       " 'reassess',\n",
       " 'ambitious',\n",
       " 'zak',\n",
       " 'pigheaded',\n",
       " 'martha',\n",
       " 'shameless',\n",
       " 'reevaluate',\n",
       " 'connect',\n",
       " 'screwed',\n",
       " 'unexpected',\n",
       " 'largely',\n",
       " 'pride',\n",
       " 'jenny',\n",
       " 'twins',\n",
       " 'feb',\n",
       " 'odds',\n",
       " 'lovable',\n",
       " 'appearance',\n",
       " 'ten',\n",
       " 'scholarship',\n",
       " 'revelling',\n",
       " 'inferiority',\n",
       " 'awake',\n",
       " 'burner',\n",
       " 'naked',\n",
       " 'sweater',\n",
       " 'bresca',\n",
       " 'nerve',\n",
       " 'transparent',\n",
       " 'cheaper',\n",
       " 'feelin',\n",
       " 'academic',\n",
       " 'regardless',\n",
       " 'ironically',\n",
       " 'dirt',\n",
       " 'remembering',\n",
       " 'blink',\n",
       " 'crab',\n",
       " 'scattered',\n",
       " 'cling',\n",
       " 'imprisoned',\n",
       " 'clients',\n",
       " 'drastic',\n",
       " 'tells',\n",
       " 'funeral',\n",
       " 'million',\n",
       " 'christmassy',\n",
       " 'procedure',\n",
       " 'stability',\n",
       " 'trendy',\n",
       " 'tuned',\n",
       " 'lunch',\n",
       " 'adoration',\n",
       " 'bipolar',\n",
       " 'destroyed',\n",
       " 'sxual',\n",
       " 'jane',\n",
       " 'chiropractor',\n",
       " 'rushing',\n",
       " 'ether',\n",
       " 'deadlines',\n",
       " 'respond',\n",
       " 'workout',\n",
       " 'comparison',\n",
       " 'recognize',\n",
       " 'dreams',\n",
       " 'returning',\n",
       " 'belonged',\n",
       " 'treize',\n",
       " 'minder',\n",
       " 'stroke',\n",
       " 'visual',\n",
       " 'undecision',\n",
       " 'aircon',\n",
       " 'objectively',\n",
       " 'identified',\n",
       " 'corsets',\n",
       " 'hips',\n",
       " 'flattered',\n",
       " 'emphasis',\n",
       " 'uttered',\n",
       " 'floaty',\n",
       " 'behaviors',\n",
       " 'measurable',\n",
       " 'tough',\n",
       " 'assuming',\n",
       " 'frenzy',\n",
       " 'fry',\n",
       " 'trend',\n",
       " 'theyll',\n",
       " 'eight',\n",
       " 'vow',\n",
       " 'mates',\n",
       " 'band',\n",
       " 'fourth',\n",
       " 'plotted',\n",
       " 'discover',\n",
       " 'proximity',\n",
       " 'dinosaur',\n",
       " 'choke',\n",
       " 'reps',\n",
       " 'wrenching',\n",
       " 'epic',\n",
       " 'cry',\n",
       " 'opinion',\n",
       " 'cewah',\n",
       " 'queue',\n",
       " 'shaped',\n",
       " 'shoppers',\n",
       " 'denali',\n",
       " 'tiredness',\n",
       " 'clinic',\n",
       " 'elses',\n",
       " 'anybody',\n",
       " 'kiss',\n",
       " 'careful',\n",
       " 'decidedly',\n",
       " 'transform',\n",
       " 'monster',\n",
       " 'bec',\n",
       " 'upbeat',\n",
       " 'estimate',\n",
       " 'announce',\n",
       " 'training',\n",
       " 'non',\n",
       " 'offence',\n",
       " 'cheeks',\n",
       " 'wp',\n",
       " 'involved',\n",
       " 'smirnoff',\n",
       " 'depending',\n",
       " 'lanres',\n",
       " 'appealing',\n",
       " 'immediate',\n",
       " 'regional',\n",
       " 'drawing',\n",
       " 'networking',\n",
       " 'pam',\n",
       " 'gordan',\n",
       " 'projectiles',\n",
       " 'quo',\n",
       " 'dir',\n",
       " 'starving',\n",
       " 'bulgarian',\n",
       " 'practically',\n",
       " 'slurred',\n",
       " 'startled',\n",
       " 'colours',\n",
       " 'anchorage',\n",
       " 'crapping',\n",
       " 'pluck',\n",
       " 'size',\n",
       " 'perfectly',\n",
       " 'sea',\n",
       " 'faisal',\n",
       " 'tucked',\n",
       " 'nipples',\n",
       " 'rescuing',\n",
       " 'meters',\n",
       " 'innocents',\n",
       " 'irregardless',\n",
       " 'iriss',\n",
       " 'spain',\n",
       " 'lines',\n",
       " 'eateries',\n",
       " 'detail',\n",
       " 'bewildered',\n",
       " 'underappreciated',\n",
       " 'meaningful',\n",
       " 'expected',\n",
       " 'racking',\n",
       " 'original',\n",
       " 'loudly',\n",
       " 'intricate',\n",
       " 'concert',\n",
       " 'suspect',\n",
       " 'scene',\n",
       " 'flung',\n",
       " 'stunning',\n",
       " 'creators',\n",
       " 'obvious',\n",
       " 'declaring',\n",
       " 'wow',\n",
       " 'studies',\n",
       " 'supposingly',\n",
       " 'plenty',\n",
       " 'letdown',\n",
       " 'maturing',\n",
       " 'zechs',\n",
       " 'shooting',\n",
       " 'unsettled',\n",
       " 'religious',\n",
       " 'haunted',\n",
       " 'edition',\n",
       " 'va',\n",
       " 'pretending',\n",
       " 'volunteers',\n",
       " 'vibration',\n",
       " 'burning',\n",
       " 'epilogue',\n",
       " 'jesus',\n",
       " 'solely',\n",
       " 'abduct',\n",
       " 'furious',\n",
       " 'insights',\n",
       " 'broken',\n",
       " 'yo',\n",
       " 'setting',\n",
       " 'flavor',\n",
       " 'tastes',\n",
       " 'volunteered',\n",
       " 'nfeel',\n",
       " 'austen',\n",
       " 'available',\n",
       " 'devastating',\n",
       " 'nao',\n",
       " 'update',\n",
       " 'sack',\n",
       " 'symp',\n",
       " 'mormons',\n",
       " 'curse',\n",
       " 'hockey',\n",
       " 'adjusts',\n",
       " 'expresses',\n",
       " 'disgusting',\n",
       " 'disney',\n",
       " 'haahaha',\n",
       " 'dids',\n",
       " 'capacity',\n",
       " 'midway',\n",
       " 'handset',\n",
       " 'european',\n",
       " 'meat',\n",
       " 'omnivore',\n",
       " 'mattered',\n",
       " 'crumbling',\n",
       " 'journal',\n",
       " 'weightlessness',\n",
       " 'forming',\n",
       " 'onions',\n",
       " 'onerous',\n",
       " 'san',\n",
       " 'sa',\n",
       " 'rant',\n",
       " 'unison',\n",
       " 'stolen',\n",
       " 'pill',\n",
       " 'midst',\n",
       " 'notice',\n",
       " 'squirts',\n",
       " 'fernando',\n",
       " 'lauging',\n",
       " 'removed',\n",
       " 'talks',\n",
       " 'reviewed',\n",
       " 'cuz',\n",
       " 'aiming',\n",
       " 'emerge',\n",
       " 'lazy',\n",
       " 'royally',\n",
       " 'feet',\n",
       " 'bowman',\n",
       " 'transformation',\n",
       " 'hassled',\n",
       " 'cheek',\n",
       " 'nikolas',\n",
       " 'asks',\n",
       " 'answered',\n",
       " 'forgetful',\n",
       " 'performance',\n",
       " 'boyfriend',\n",
       " 'scandalised',\n",
       " 'meaning',\n",
       " 'receiver',\n",
       " 'courses',\n",
       " 'condescending',\n",
       " 'fears',\n",
       " 'repetition',\n",
       " 'population',\n",
       " 'fog',\n",
       " 'quoted',\n",
       " 'quests',\n",
       " 'string',\n",
       " 'woundering',\n",
       " 'hindi',\n",
       " 'directly',\n",
       " 'raised',\n",
       " 'yell',\n",
       " 'desolation',\n",
       " 'advice',\n",
       " 'fantasy',\n",
       " 'rhetoric',\n",
       " 'screw',\n",
       " 'mysterious',\n",
       " 'safely',\n",
       " 'conceptually',\n",
       " 'industry',\n",
       " 'offered',\n",
       " 'entities',\n",
       " 'crisis',\n",
       " 'ltr',\n",
       " 'liberation',\n",
       " 'lash',\n",
       " 'melissa',\n",
       " 'tall',\n",
       " 'childish',\n",
       " 'ton',\n",
       " 'service',\n",
       " 'pest',\n",
       " 'reign',\n",
       " 'hateable',\n",
       " 'lung',\n",
       " 'lake',\n",
       " 'announcement',\n",
       " 'jins',\n",
       " 'college',\n",
       " 'surrendered',\n",
       " 'chizuru',\n",
       " 'nga',\n",
       " 'sample',\n",
       " 'lap',\n",
       " 'orgasm',\n",
       " 'bonze',\n",
       " 'scales',\n",
       " 'dyed',\n",
       " 'breakfast',\n",
       " 'shot',\n",
       " 'luggage',\n",
       " 'disease',\n",
       " 'blackness',\n",
       " 'overs',\n",
       " 'concerns',\n",
       " 'satire',\n",
       " 'method',\n",
       " 'newer',\n",
       " 'compulsion',\n",
       " 'destroying',\n",
       " 'akari',\n",
       " 'descriptive',\n",
       " 'kaixiang',\n",
       " 'neo',\n",
       " 'notion',\n",
       " 'suffocated',\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_uniquewords - train_uniquewords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "826b6ea3-0910-47c7-9cf6-8adc652a675b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'isolating',\n",
       " 'hairdresser',\n",
       " 'cost',\n",
       " 'distant',\n",
       " 'fault',\n",
       " 'bodyworks',\n",
       " 'beneath',\n",
       " 'resurrection',\n",
       " 'grub',\n",
       " 'dead',\n",
       " 'slepted',\n",
       " 'sincerity',\n",
       " 'pairing',\n",
       " 'designer',\n",
       " 'petted',\n",
       " 'sunlight',\n",
       " 'handful',\n",
       " 'shitting',\n",
       " 'stillness',\n",
       " 'immensely',\n",
       " 'shouting',\n",
       " 'handle',\n",
       " 'apparently',\n",
       " 'challenges',\n",
       " 'alarmed',\n",
       " 'revenue',\n",
       " 'ties',\n",
       " 'acts',\n",
       " 'abolutely',\n",
       " 'cop',\n",
       " 'maternity',\n",
       " 'challenging',\n",
       " 'shivering',\n",
       " 'collection',\n",
       " 'commandments',\n",
       " 'wishlist',\n",
       " 'planning',\n",
       " 'fence',\n",
       " 'righting',\n",
       " 'greatness',\n",
       " 'hiccups',\n",
       " 'traffic',\n",
       " 'shown',\n",
       " 'swings',\n",
       " 'reversed',\n",
       " 'covering',\n",
       " 'species',\n",
       " 'resistant',\n",
       " 'belief',\n",
       " 'intended',\n",
       " 'beginnings',\n",
       " 'cons',\n",
       " 'nowhere',\n",
       " 'sports',\n",
       " 'wore',\n",
       " 'tactical',\n",
       " 'gifted',\n",
       " 'storm',\n",
       " 'arrange',\n",
       " 'idiots',\n",
       " 'sometime',\n",
       " 'homeschool',\n",
       " 'beast',\n",
       " 'cassiopeia',\n",
       " 'amy',\n",
       " 'option',\n",
       " 'drawn',\n",
       " 'bob',\n",
       " 'wholeness',\n",
       " 'monthly',\n",
       " 'occur',\n",
       " 'unlovable',\n",
       " 'discount',\n",
       " 'healthier',\n",
       " 'connection',\n",
       " 'consultation',\n",
       " 'lawn',\n",
       " 'bw',\n",
       " 'buoied',\n",
       " 'heaven',\n",
       " 'inconsiderate',\n",
       " 'invitation',\n",
       " 'impolite',\n",
       " 'hai',\n",
       " 'closest',\n",
       " 'sincerely',\n",
       " 'frances',\n",
       " 'brace',\n",
       " 'routine',\n",
       " 'granted',\n",
       " 'environment',\n",
       " 'adds',\n",
       " 'green',\n",
       " 'dried',\n",
       " 'porch',\n",
       " 'daily',\n",
       " 'runner',\n",
       " 'guitar',\n",
       " 'indestructible',\n",
       " 'distracted',\n",
       " 'gently',\n",
       " 'thailand',\n",
       " 'relative',\n",
       " 'buying',\n",
       " 'international',\n",
       " 'simmons',\n",
       " 'organised',\n",
       " 'trauma',\n",
       " 'headaches',\n",
       " 'remaking',\n",
       " 'table',\n",
       " 'validations',\n",
       " 'ingredients',\n",
       " 'practices',\n",
       " 'masturbate',\n",
       " 'schooling',\n",
       " 'behave',\n",
       " 'exiting',\n",
       " 'invited',\n",
       " 'fab',\n",
       " 'enraged',\n",
       " 'responding',\n",
       " 'potencial',\n",
       " 'changed',\n",
       " 'onto',\n",
       " 'wing',\n",
       " 'ol',\n",
       " 'drill',\n",
       " 'hopefulness',\n",
       " 'purpose',\n",
       " 'veins',\n",
       " 'seeking',\n",
       " 'prop',\n",
       " 'indulge',\n",
       " 'repharse',\n",
       " 'interact',\n",
       " 'affection',\n",
       " 'captures',\n",
       " 'evenly',\n",
       " 'conformity',\n",
       " 'battle',\n",
       " 'ache',\n",
       " 'thicker',\n",
       " 'xanax',\n",
       " 'insignificant',\n",
       " 'signalled',\n",
       " 'hop',\n",
       " 'frustrating',\n",
       " 'emailed',\n",
       " 'shagging',\n",
       " 'disbelief',\n",
       " 'blew',\n",
       " 'commericals',\n",
       " 'lies',\n",
       " 'gotta',\n",
       " 'missions',\n",
       " 'misunderstanding',\n",
       " 'enslave',\n",
       " 'shorts',\n",
       " 'threats',\n",
       " 'aberdeen',\n",
       " 'example',\n",
       " 'coping',\n",
       " 'dmaned',\n",
       " 'meandering',\n",
       " 'board',\n",
       " 'despised',\n",
       " 'space',\n",
       " 'badge',\n",
       " 'unfollows',\n",
       " 'dictate',\n",
       " 'bacon',\n",
       " 'experimented',\n",
       " 'material',\n",
       " 'leech',\n",
       " 'scrapyard',\n",
       " 'countries',\n",
       " 'revered',\n",
       " 'seven',\n",
       " 'justifications',\n",
       " 'common',\n",
       " 'company',\n",
       " 'error',\n",
       " 'tucking',\n",
       " 'doses',\n",
       " 'piglet',\n",
       " 'include',\n",
       " 'murmur',\n",
       " 'shouldn',\n",
       " 'ought',\n",
       " 'scent',\n",
       " 'female',\n",
       " 'waddled',\n",
       " 'heartburn',\n",
       " 'sexy',\n",
       " 'entailed',\n",
       " 'puppets',\n",
       " 'sores',\n",
       " 'associated',\n",
       " 'generators',\n",
       " 'uninspired',\n",
       " 'accent',\n",
       " 'offensively',\n",
       " 'iv',\n",
       " 'hilltoppers',\n",
       " 'perseverance',\n",
       " 'twitter',\n",
       " 'royal',\n",
       " 'melt',\n",
       " 'barely',\n",
       " 'docters',\n",
       " 'collie',\n",
       " 'pleasures',\n",
       " 'attacked',\n",
       " 'legolasses',\n",
       " 'portrait',\n",
       " 'scale',\n",
       " 'distrustful',\n",
       " 'surrounds',\n",
       " 'timelessness',\n",
       " 'holder',\n",
       " 'wings',\n",
       " 'opposite',\n",
       " 'act',\n",
       " 'themes',\n",
       " 'hardly',\n",
       " 'side',\n",
       " 'impending',\n",
       " 'bugging',\n",
       " 'cameras',\n",
       " 'homeschooling',\n",
       " 'associate',\n",
       " 'silkyway',\n",
       " 'roadside',\n",
       " 'lust',\n",
       " 'weekly',\n",
       " 'younger',\n",
       " 'weighing',\n",
       " 'mayra',\n",
       " 'frump',\n",
       " 'pile',\n",
       " 'stops',\n",
       " 'finished',\n",
       " 'deliverables',\n",
       " 'field',\n",
       " 'seeping',\n",
       " 'valentines',\n",
       " 'achieve',\n",
       " 'csu',\n",
       " 'tourism',\n",
       " 'handling',\n",
       " 'released',\n",
       " 'acceptance',\n",
       " 'upwards',\n",
       " 'flu',\n",
       " 'introduce',\n",
       " 'refused',\n",
       " 'question',\n",
       " 'douche',\n",
       " 'require',\n",
       " 'conclusions',\n",
       " 'falling',\n",
       " 'ends',\n",
       " 'max',\n",
       " 'entertained',\n",
       " 'indismissable',\n",
       " 'targeted',\n",
       " 'appetizer',\n",
       " 'hearing',\n",
       " 'sympathy',\n",
       " 'concerned',\n",
       " 'fn',\n",
       " 'jonah',\n",
       " 'satisfaction',\n",
       " 'downright',\n",
       " 'threatened',\n",
       " 'football',\n",
       " 'permanence',\n",
       " 'attended',\n",
       " 'resign',\n",
       " 'teams',\n",
       " 'fortunes',\n",
       " 'discovered',\n",
       " 'peculiarly',\n",
       " 'tame',\n",
       " 'nudge',\n",
       " 'ballot',\n",
       " 'succeeded',\n",
       " 'fared',\n",
       " 'approach',\n",
       " 'flag',\n",
       " 'yah',\n",
       " 'settle',\n",
       " 'honesty',\n",
       " 'yihan',\n",
       " 'jolly',\n",
       " 'showed',\n",
       " 'cookie',\n",
       " 'captivating',\n",
       " 'aspects',\n",
       " 'hill',\n",
       " 'stitch',\n",
       " 'slight',\n",
       " 'manage',\n",
       " 'yai',\n",
       " 'lips',\n",
       " 'protest',\n",
       " 'unfortunately',\n",
       " 'sinking',\n",
       " 'morally',\n",
       " 'castiel',\n",
       " 'fight',\n",
       " 'continues',\n",
       " 'beat',\n",
       " 'comment',\n",
       " 'glass',\n",
       " 'laura',\n",
       " 'colucci',\n",
       " 'electrician',\n",
       " 'fix',\n",
       " 'restless',\n",
       " 'nurse',\n",
       " 'keenly',\n",
       " 'engaging',\n",
       " 'nude',\n",
       " 'unvalidated',\n",
       " 'cozy',\n",
       " 'begs',\n",
       " 'suffer',\n",
       " 'details',\n",
       " 'pint',\n",
       " 'bounce',\n",
       " 'reactions',\n",
       " 'demo',\n",
       " 'becuase',\n",
       " 'yielded',\n",
       " 'eye',\n",
       " 'pounds',\n",
       " 'overwhelm',\n",
       " 'flowers',\n",
       " 'indicator',\n",
       " 'cirlce',\n",
       " 'isn',\n",
       " 'packing',\n",
       " 'treadmill',\n",
       " 'completed',\n",
       " 'nails',\n",
       " 'worn',\n",
       " 'expression',\n",
       " 'motorcycle',\n",
       " 'flows',\n",
       " 'yelps',\n",
       " 'path',\n",
       " 'als',\n",
       " 'beauty',\n",
       " 'afternoon',\n",
       " 'talkative',\n",
       " 'anmd',\n",
       " 'duelists',\n",
       " 'knowledgeable',\n",
       " 'strikers',\n",
       " 'inner',\n",
       " 'saturday',\n",
       " 'fbi',\n",
       " 'director',\n",
       " 'theres',\n",
       " 'accessories',\n",
       " 'desperately',\n",
       " 'editor',\n",
       " 'car',\n",
       " 'acclimatized',\n",
       " 'previous',\n",
       " 'doug',\n",
       " 'prom',\n",
       " 'bowl',\n",
       " 'basis',\n",
       " 'harder',\n",
       " 'automatically',\n",
       " 'opted',\n",
       " 'latin',\n",
       " 'concern',\n",
       " 'types',\n",
       " 'shorter',\n",
       " 'unimportant',\n",
       " 'vessel',\n",
       " 'unsaved',\n",
       " 'weep',\n",
       " 'sarcastic',\n",
       " 'members',\n",
       " 'sewing',\n",
       " 'tinkle',\n",
       " 'edge',\n",
       " 'regularly',\n",
       " 'assignment',\n",
       " 'easier',\n",
       " 'boat',\n",
       " 'deflated',\n",
       " 'gratitude',\n",
       " 'jv',\n",
       " 'ourselves',\n",
       " 'dressed',\n",
       " 'norm',\n",
       " 'advantage',\n",
       " 'chik',\n",
       " 'reply',\n",
       " 'myth',\n",
       " 'solemn',\n",
       " 'cared',\n",
       " 'ishihara',\n",
       " 'page',\n",
       " 'solution',\n",
       " 'engineers',\n",
       " 'exhaustion',\n",
       " 'organize',\n",
       " 'rap',\n",
       " 'meet',\n",
       " 'spoken',\n",
       " 'appalled',\n",
       " 'peyton',\n",
       " 'actively',\n",
       " 'nse',\n",
       " 'performed',\n",
       " 'nikki',\n",
       " 'sociable',\n",
       " 'hopeful',\n",
       " 'gain',\n",
       " 'comfort',\n",
       " 'parked',\n",
       " 'accents',\n",
       " 'stack',\n",
       " 'smarter',\n",
       " 'further',\n",
       " 'package',\n",
       " 'resolution',\n",
       " 'scouting',\n",
       " 'necessary',\n",
       " 'gratefulness',\n",
       " 'musical',\n",
       " 'empathetic',\n",
       " 'attacking',\n",
       " 'bug',\n",
       " 'medicine',\n",
       " 'hairs',\n",
       " 'near',\n",
       " 'snappish',\n",
       " 'interviews',\n",
       " 'blurry',\n",
       " 'dedicated',\n",
       " 'vanessa',\n",
       " 'contributors',\n",
       " 'depression',\n",
       " 'fingertips',\n",
       " 'imagination',\n",
       " 'loser',\n",
       " 'intense',\n",
       " 'cheese',\n",
       " 'players',\n",
       " 'chaotic',\n",
       " 'marker',\n",
       " 'emmotionally',\n",
       " 'greet',\n",
       " 'concerted',\n",
       " 'extraordinarily',\n",
       " 'joe',\n",
       " 'surround',\n",
       " 'thrown',\n",
       " 'attraction',\n",
       " 'student',\n",
       " 'blogged',\n",
       " 'drugs',\n",
       " 'experiencing',\n",
       " 'benefiting',\n",
       " 'takes',\n",
       " 'vary',\n",
       " 'boring',\n",
       " 'ship',\n",
       " 'repulsed',\n",
       " 'resolutions',\n",
       " 'site',\n",
       " 'instantly',\n",
       " 'harrison',\n",
       " 'sorrow',\n",
       " 'wanderers',\n",
       " 'happening',\n",
       " 'glare',\n",
       " 'drives',\n",
       " 'mamen',\n",
       " 'haze',\n",
       " 'ladys',\n",
       " 'journals',\n",
       " 'notes',\n",
       " 'helper',\n",
       " 'readings',\n",
       " 'limbo',\n",
       " 'justice',\n",
       " 'ooh',\n",
       " 'fun',\n",
       " 'struggle',\n",
       " 'idd',\n",
       " 'hadn',\n",
       " 'messages',\n",
       " 'tied',\n",
       " 'wedding',\n",
       " 'wizard',\n",
       " 'send',\n",
       " 'gary',\n",
       " 'entered',\n",
       " 'transitions',\n",
       " 'practice',\n",
       " 'screwing',\n",
       " 'national',\n",
       " 'miles',\n",
       " 'peoples',\n",
       " 'itd',\n",
       " 'clear',\n",
       " 'anti',\n",
       " 'brthers',\n",
       " 'abysmally',\n",
       " 'knowng',\n",
       " 'ali',\n",
       " 'petition',\n",
       " 'advair',\n",
       " 'thanks',\n",
       " 'dancing',\n",
       " 'examples',\n",
       " 'value',\n",
       " 'annoying',\n",
       " 'callllm',\n",
       " 'access',\n",
       " 'role',\n",
       " 'pics',\n",
       " 'association',\n",
       " 'arguments',\n",
       " 'yuchun',\n",
       " 'survival',\n",
       " 'receive',\n",
       " 'snuggling',\n",
       " 'cleaned',\n",
       " 'reaping',\n",
       " 'radiohead',\n",
       " 'portion',\n",
       " 'cookies',\n",
       " 'pretended',\n",
       " 'spending',\n",
       " 'bader',\n",
       " 'entry',\n",
       " 'heartbreaking',\n",
       " 'compassionate',\n",
       " 'hall',\n",
       " 'measurements',\n",
       " 'technicalities',\n",
       " 'scheduled',\n",
       " 'extra',\n",
       " 'savior',\n",
       " 'itsself',\n",
       " 'alr',\n",
       " 'palm',\n",
       " 'socialize',\n",
       " 'sooo',\n",
       " 'june',\n",
       " 'symptoms',\n",
       " 'kak',\n",
       " 'tbr',\n",
       " 'tread',\n",
       " 'organization',\n",
       " 'hugely',\n",
       " 'oat',\n",
       " 'tweeple',\n",
       " 'camping',\n",
       " 'third',\n",
       " 'fed',\n",
       " 'aspirations',\n",
       " 'exhibitionist',\n",
       " 'saints',\n",
       " 'documenting',\n",
       " 'unopposed',\n",
       " 'crosswords',\n",
       " 'fat',\n",
       " 'burton',\n",
       " 'converse',\n",
       " 'ago',\n",
       " 'accompanying',\n",
       " 'materials',\n",
       " 'boggles',\n",
       " 'ludicrous',\n",
       " 'intelligence',\n",
       " 'reciprecate',\n",
       " 'bread',\n",
       " 'mission',\n",
       " 'spaced',\n",
       " 'intruding',\n",
       " 'bunch',\n",
       " 'wondered',\n",
       " 'amicable',\n",
       " 'marvel',\n",
       " 'explicit',\n",
       " 'jols',\n",
       " 'amateur',\n",
       " 'bike',\n",
       " 'nurtured',\n",
       " 'gosh',\n",
       " 'verbalize',\n",
       " 'sondheim',\n",
       " 'duality',\n",
       " 'nonetheless',\n",
       " 'florals',\n",
       " 'cautiously',\n",
       " 'nov',\n",
       " 'kisses',\n",
       " 'blinder',\n",
       " 'stadium',\n",
       " 'prac',\n",
       " 'bachelorette',\n",
       " 'spreading',\n",
       " 'macaron',\n",
       " 'acne',\n",
       " 'rebuilding',\n",
       " 'safer',\n",
       " 'pros',\n",
       " 'tape',\n",
       " 'sickened',\n",
       " 'kirk',\n",
       " 'furthest',\n",
       " 'denial',\n",
       " 'folds',\n",
       " 'kiev',\n",
       " 'greenery',\n",
       " 'advisable',\n",
       " 'traveled',\n",
       " 'analyse',\n",
       " 'loosen',\n",
       " 'sis',\n",
       " 'wet',\n",
       " 'assuring',\n",
       " 'emotionally',\n",
       " 'system',\n",
       " 'article',\n",
       " 'bath',\n",
       " 'commit',\n",
       " 'fire',\n",
       " 'chris',\n",
       " 'tear',\n",
       " 'dependency',\n",
       " 'answer',\n",
       " 'mama',\n",
       " 'arms',\n",
       " 'favorite',\n",
       " 'flip',\n",
       " 'par',\n",
       " 'january',\n",
       " 'nauseous',\n",
       " 'involuntary',\n",
       " 'guessing',\n",
       " 'mthers',\n",
       " 'gardener',\n",
       " 'balcony',\n",
       " 'ngah',\n",
       " 'sober',\n",
       " 'biker',\n",
       " 'boss',\n",
       " 'foretell',\n",
       " 'opportunities',\n",
       " 'nicholas',\n",
       " 'hunger',\n",
       " 'rusty',\n",
       " 'incredible',\n",
       " 'distance',\n",
       " 'cope',\n",
       " 'envision',\n",
       " 'guidance',\n",
       " 'plastic',\n",
       " 'annual',\n",
       " 'hoped',\n",
       " 'happened',\n",
       " 'ignore',\n",
       " 'regular',\n",
       " 'thyroid',\n",
       " 'bathtub',\n",
       " 'india',\n",
       " 'womb',\n",
       " 'wood',\n",
       " 'toff',\n",
       " 'deciding',\n",
       " 'points',\n",
       " 'separately',\n",
       " 'interning',\n",
       " 'plays',\n",
       " 'frolicking',\n",
       " 'ones',\n",
       " 'boards',\n",
       " 'weights',\n",
       " 'statue',\n",
       " 'heartache',\n",
       " 'library',\n",
       " 'neglectful',\n",
       " 'opposed',\n",
       " 'facilitate',\n",
       " 'canadians',\n",
       " 'swim',\n",
       " 'incidents',\n",
       " 'accomplishments',\n",
       " 'dispel',\n",
       " 'depth',\n",
       " 'cutting',\n",
       " 'mother',\n",
       " 'chills',\n",
       " 'photo',\n",
       " 'hypothermia',\n",
       " 'math',\n",
       " 'modling',\n",
       " 'primarily',\n",
       " 'jamie',\n",
       " 'egocentric',\n",
       " 'queers',\n",
       " 'frankly',\n",
       " 'lush',\n",
       " 'sadly',\n",
       " 'winced',\n",
       " 'evoking',\n",
       " 'imposed',\n",
       " 'paintings',\n",
       " 'aware',\n",
       " 'overwhelming',\n",
       " 'losing',\n",
       " 'lecture',\n",
       " 'tmsi',\n",
       " 'kareena',\n",
       " 'huhu',\n",
       " 'influential',\n",
       " 'pilled',\n",
       " 'observed',\n",
       " 'fave',\n",
       " 'movement',\n",
       " 'snobbish',\n",
       " 'academia',\n",
       " 'luck',\n",
       " 'joyfully',\n",
       " 'substantially',\n",
       " 'whisper',\n",
       " 'accomplished',\n",
       " 'cardinals',\n",
       " 'figure',\n",
       " 'knowing',\n",
       " 'fruit',\n",
       " 'weeping',\n",
       " 'carefully',\n",
       " 'nametags',\n",
       " 'nights',\n",
       " 'celebrity',\n",
       " 'consume',\n",
       " 'noddy',\n",
       " 'upright',\n",
       " 'consultations',\n",
       " 'img',\n",
       " 'unfinished',\n",
       " 'beliefs',\n",
       " 'predicted',\n",
       " 'surface',\n",
       " 'processing',\n",
       " 'succeed',\n",
       " 'daunting',\n",
       " 'profound',\n",
       " 'basic',\n",
       " 'kai',\n",
       " 'gravitate',\n",
       " 'positively',\n",
       " 'repressed',\n",
       " 'busting',\n",
       " 'camp',\n",
       " 'whiney',\n",
       " 'hospitality',\n",
       " 'babyness',\n",
       " 'opening',\n",
       " 'sin',\n",
       " 'hit',\n",
       " 'pd',\n",
       " 'aggressive',\n",
       " 'rainfall',\n",
       " 'dealin',\n",
       " 'functions',\n",
       " 'february',\n",
       " 'bet',\n",
       " 'causes',\n",
       " 'fertilized',\n",
       " 'faster',\n",
       " 'thus',\n",
       " 'date',\n",
       " 'mouth',\n",
       " 'junsu',\n",
       " 'harsh',\n",
       " 'percent',\n",
       " 'metabolism',\n",
       " 'roadhouse',\n",
       " 'crash',\n",
       " 'carrying',\n",
       " 'sing',\n",
       " 'outfit',\n",
       " 'bella',\n",
       " 'teaching',\n",
       " 'downsides',\n",
       " 'legolas',\n",
       " 'laugh',\n",
       " 'disapproval',\n",
       " 'techthings',\n",
       " 'fan',\n",
       " 'nicely',\n",
       " 'arrogant',\n",
       " 'humanity',\n",
       " 'protecting',\n",
       " 'sitcoms',\n",
       " 'cross',\n",
       " 'informed',\n",
       " 'reliable',\n",
       " 'gifts',\n",
       " 'eh',\n",
       " 'kale',\n",
       " 'drug',\n",
       " 'persuasions',\n",
       " 'monetary',\n",
       " 'assemble',\n",
       " 'sparks',\n",
       " 'grshino',\n",
       " 'arita',\n",
       " 'mercy',\n",
       " 'khan',\n",
       " 'solved',\n",
       " 'scraped',\n",
       " 'concept',\n",
       " 'silk',\n",
       " 'goto',\n",
       " 'castle',\n",
       " 'republic',\n",
       " 'character',\n",
       " 'endeavors',\n",
       " 'orgasms',\n",
       " 'sunday',\n",
       " 'caffeine',\n",
       " 'elders',\n",
       " 'signs',\n",
       " 'conferences',\n",
       " 'tragic',\n",
       " 'swing',\n",
       " 'popular',\n",
       " 'rental',\n",
       " 'addiction',\n",
       " 'correct',\n",
       " 'kampung',\n",
       " 'heat',\n",
       " 'lk',\n",
       " 'wisely',\n",
       " 'sneering',\n",
       " 'jobs',\n",
       " 'russian',\n",
       " 'ica',\n",
       " 'hinted',\n",
       " 'borrowed',\n",
       " 'inadequate',\n",
       " 'fill',\n",
       " 'incomprehension',\n",
       " 'sorrowful',\n",
       " 'sufficed',\n",
       " 'layer',\n",
       " 'wildly',\n",
       " 'greed',\n",
       " 'albums',\n",
       " 'technologies',\n",
       " 'latter',\n",
       " 'trap',\n",
       " 'personalities',\n",
       " 'managed',\n",
       " 'card',\n",
       " 'beach',\n",
       " 'complex',\n",
       " 'mashalla',\n",
       " 'adventuresofjkl',\n",
       " 'perform',\n",
       " 'damaging',\n",
       " 'desires',\n",
       " 'stronger',\n",
       " 'tightness',\n",
       " 'capture',\n",
       " 'emergency',\n",
       " 'cheated',\n",
       " 'lifting',\n",
       " 'censor',\n",
       " 'brodens',\n",
       " 'arent',\n",
       " 'proverbial',\n",
       " 'doorway',\n",
       " 'internal',\n",
       " 'ice',\n",
       " 'fuck',\n",
       " 'helm',\n",
       " 'altitude',\n",
       " 'handed',\n",
       " 'prayed',\n",
       " 'begging',\n",
       " 'wife',\n",
       " 'whip',\n",
       " 'mortals',\n",
       " 'interior',\n",
       " 'improve',\n",
       " 'miyavi',\n",
       " 'ungodly',\n",
       " 'whatnot',\n",
       " 'displaced',\n",
       " 'album',\n",
       " 'ducation',\n",
       " 'zpg',\n",
       " 'wasting',\n",
       " 'temptation',\n",
       " 'forbidden',\n",
       " 'peter',\n",
       " 'deepest',\n",
       " 'poisoned',\n",
       " 'becomes',\n",
       " 'retire',\n",
       " 'spine',\n",
       " 'manifest',\n",
       " 'sleeping',\n",
       " 'inhumane',\n",
       " 'replace',\n",
       " 'emissions',\n",
       " 'erratic',\n",
       " 'brands',\n",
       " 'none',\n",
       " 'derision',\n",
       " 'overwork',\n",
       " 'screaming',\n",
       " 'boooo',\n",
       " 'budgeting',\n",
       " 'angels',\n",
       " 'initiates',\n",
       " 'phases',\n",
       " 'itll',\n",
       " 'backs',\n",
       " 'dc',\n",
       " 'nourishing',\n",
       " 'officer',\n",
       " 'legitimately',\n",
       " 'materializing',\n",
       " 'alright',\n",
       " 'humbled',\n",
       " 'store',\n",
       " 'pull',\n",
       " 'shell',\n",
       " 'astray',\n",
       " 'roommate',\n",
       " 'iam',\n",
       " 'spreads',\n",
       " 'bricks',\n",
       " 'enjoyable',\n",
       " 'sequel',\n",
       " 'forgiven',\n",
       " 'included',\n",
       " 'cops',\n",
       " 'cutters',\n",
       " 'nowadays',\n",
       " 'corrupt',\n",
       " 'enforcers',\n",
       " 'messing',\n",
       " 'faceplate',\n",
       " 'toss',\n",
       " 'creates',\n",
       " 'fragile',\n",
       " 'withering',\n",
       " 'departed',\n",
       " 'painful',\n",
       " 'swirling',\n",
       " 'keane',\n",
       " 'spirutally',\n",
       " 'badass',\n",
       " 'impassive',\n",
       " 'havnt',\n",
       " 'beck',\n",
       " 'christ',\n",
       " 'cow',\n",
       " 'bible',\n",
       " 'worthlessness',\n",
       " 'lah',\n",
       " 'calming',\n",
       " 'unplugged',\n",
       " 'pomp',\n",
       " 'colgate',\n",
       " 'struggling',\n",
       " 'disappointment',\n",
       " 'photograph',\n",
       " 'everytime',\n",
       " 'crazy',\n",
       " 'outings',\n",
       " 'grind',\n",
       " 'pillars',\n",
       " 'mow',\n",
       " 'buddy',\n",
       " 'dentures',\n",
       " 'mermaid',\n",
       " 'lettuce',\n",
       " 'grasp',\n",
       " 'fighting',\n",
       " 'sir',\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_uniquewords - test_uniquewords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d6cf995-6312-4bbb-b059-9a5db77b9942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27383</td>\n",
       "      <td>i feel awful about it too because it s my job ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110083</td>\n",
       "      <td>im alone i feel awful</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140764</td>\n",
       "      <td>ive probably mentioned this before but i reall...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100071</td>\n",
       "      <td>i was feeling a little low few days back</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2837</td>\n",
       "      <td>i beleive that i am much more sensitive to oth...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>40054</td>\n",
       "      <td>i was feeling terrified and anxious about ever...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>104110</td>\n",
       "      <td>i was tempted to feel a little depressed about...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>106240</td>\n",
       "      <td>i wish i had done things differently miss the ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>5483</td>\n",
       "      <td>i feel more and more curious anxious to see me...</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>101994</td>\n",
       "      <td>i was feeling so ungrateful earlier this week</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  emotions\n",
       "0      27383  i feel awful about it too because it s my job ...   sadness\n",
       "1     110083                              im alone i feel awful   sadness\n",
       "2     140764  ive probably mentioned this before but i reall...       joy\n",
       "3     100071           i was feeling a little low few days back   sadness\n",
       "4       2837  i beleive that i am much more sensitive to oth...      love\n",
       "...      ...                                                ...       ...\n",
       "1196   40054  i was feeling terrified and anxious about ever...      fear\n",
       "1197  104110  i was tempted to feel a little depressed about...   sadness\n",
       "1198  106240  i wish i had done things differently miss the ...   sadness\n",
       "1199    5483  i feel more and more curious anxious to see me...  surprise\n",
       "1200  101994      i was feeling so ungrateful earlier this week   sadness\n",
       "\n",
       "[1200 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d65e3347-3851-4dad-b239-61ad4f328ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27383</td>\n",
       "      <td>i feel awful about it too because it s my job ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110083</td>\n",
       "      <td>im alone i feel awful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140764</td>\n",
       "      <td>ive probably mentioned this before but i reall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100071</td>\n",
       "      <td>i was feeling a little low few days back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2837</td>\n",
       "      <td>i beleive that i am much more sensitive to oth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>40054</td>\n",
       "      <td>i was feeling terrified and anxious about ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>104110</td>\n",
       "      <td>i was tempted to feel a little depressed about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>106240</td>\n",
       "      <td>i wish i had done things differently miss the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>5483</td>\n",
       "      <td>i feel more and more curious anxious to see me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>101994</td>\n",
       "      <td>i was feeling so ungrateful earlier this week</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text\n",
       "0      27383  i feel awful about it too because it s my job ...\n",
       "1     110083                              im alone i feel awful\n",
       "2     140764  ive probably mentioned this before but i reall...\n",
       "3     100071           i was feeling a little low few days back\n",
       "4       2837  i beleive that i am much more sensitive to oth...\n",
       "...      ...                                                ...\n",
       "1196   40054  i was feeling terrified and anxious about ever...\n",
       "1197  104110  i was tempted to feel a little depressed about...\n",
       "1198  106240  i wish i had done things differently miss the ...\n",
       "1199    5483  i feel more and more curious anxious to see me...\n",
       "1200  101994      i was feeling so ungrateful earlier this week\n",
       "\n",
       "[1200 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[[\"id\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a7a207f-ee5e-4d8f-bbe3-ea94a4c6a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_words.str.lower().str.findall(\"\\w+\")\n",
    "test_sentences = test_words.str.lower().str.findall(\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26471279-c459-4c29-a227-c5b52150a539",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ed79882-c1f1-479d-9399-883ec07915b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unknown',\n",
       " 'been',\n",
       " 'feeling',\n",
       " 'more',\n",
       " 'unknown',\n",
       " 'this',\n",
       " 'week',\n",
       " 'than',\n",
       " 'i',\n",
       " 'have',\n",
       " 'in',\n",
       " 'months']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences[0]\n",
    "replacement = [\"ive\", \"optimistic\"]\n",
    "test_sentence2 = [\"unknown\" if val in replacement else val for val in test_sentences[0]] \n",
    "test_sentence2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f39bc69-4958-4b98-9bf5-4ac98c112789",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = pd.Series(['Topic sentences are similar to mini thesis statements. Like a thesis statement', 'a topic sentence has a specific main point. Whereas the thesis is the main point of the essay, the topic sentence is the main point of the paragraph. Like the thesis statement, a topic sentence has a unifying function. But a thesis statement or topic sentence alone doesn’t guarantee unity.', 'An essay is unified if all the paragraphs relate to the thesis, whereas a paragraph is unified if all the sentences relate to the topic sentence.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4390ff97-400e-4ced-ab83-75bfe23fd9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Topic sentences are similar to mini thesis sta...\n",
       "1    a topic sentence has a specific main point. Wh...\n",
       "2    An essay is unified if all the paragraphs rela...\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fb1d71da-b265-474b-a732-420cbc2a6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF_PROCESSOR():\n",
    "    def __init__(self):\n",
    "        self.vocabulary = None\n",
    "        self.doc_count = None\n",
    "        self.doc_frequencies = None\n",
    "\n",
    "    def configure(self, data_col, threshold=1):\n",
    "        # Assumes pandas column\n",
    "        # Compute tf_idf model, to be used in new sentences.\n",
    "        # word count must be AT LEAST threshold to be accepted\n",
    "\n",
    "        # Text and split into strings\n",
    "        sentences = data_col.str.lower().str.findall(\"\\w+\")\n",
    "        self.doc_count = len(sentences)\n",
    "\n",
    "        # All unique words and their counts\n",
    "        word_counter = pd.Series(sentences.sum()).value_counts()\n",
    "\n",
    "        unknown_words = set()\n",
    "\n",
    "        for word in word_counter.keys():\n",
    "            if word_counter[word] < threshold:\n",
    "                unknown_words.add(word)\n",
    "        \n",
    "        \n",
    "        print(\"Number of words with less than \" + str(threshold) + \" occurences: \" + str(len(unknown_words)))\n",
    "\n",
    "        # Calculate word value counts\n",
    "        word_doc_frequency = dict()\n",
    "        word_doc_frequency[\"_UNKNOWN\"] = 0  # Unknown vector\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # Get series indexed by word and whose value is the number of occurences\n",
    "            sentence_word_counter = pd.Series(sentence).value_counts()\n",
    "            for word in sentence_word_counter.keys():\n",
    "                if word in unknown_words:\n",
    "                    word_doc_frequency[\"_UNKNOWN\"] += sentence_word_counter[word]\n",
    "                else:\n",
    "                    if word in word_doc_frequency:\n",
    "                        word_doc_frequency[word] += sentence_word_counter[word]\n",
    "                    else:\n",
    "                        word_doc_frequency[word] = sentence_word_counter[word]\n",
    "\n",
    "        self.doc_frequencies = word_doc_frequency\n",
    "        self.vocabulary = dict(zip(sorted(self.doc_frequencies.keys()), range(len(self.doc_frequencies.keys()))))\n",
    "\n",
    "    def apply(self, orig_sentence):\n",
    "        # determine replacement for low-frequency words\n",
    "        sentence = pd.Series(orig_sentence).str.lower().str.findall(\"\\w+\").to_list()[0]\n",
    "        vec = np.zeros((len(self.doc_frequencies.keys()),))\n",
    "\n",
    "        # Replace with unknowns whenever valid.\n",
    "        for i in range(len(sentence)):\n",
    "            if sentence[i] not in self.doc_frequencies.keys():\n",
    "                sentence[i] = \"_UNKNOWN\"\n",
    "\n",
    "        # Configure term frequency\n",
    "        term_frequencies = dict(pd.Series(sentence).value_counts() / len(sentence))\n",
    "\n",
    "        vectors = np.zeros(len(self.vocabulary.keys()))\n",
    "        # Implementation of tf_idf\n",
    "        for word in term_frequencies.keys():\n",
    "            tf = term_frequencies[word]\n",
    "            count = self.doc_frequencies[word] + 1\n",
    "            idf = np.log(self.doc_count / count)\n",
    "            position = self.vocabulary[word]\n",
    "            vectors[position] = tf * idf\n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4a2cdc13-2a1f-48b5-ac69-eb7b50786a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with less than 2 occurences: 2127\n"
     ]
    }
   ],
   "source": [
    "tf_idf_class = TFIDF_PROCESSOR()\n",
    "tf_idf_class.configure(train_ds[\"text\"], threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2d467eef-5764-4c8c-8698-484d7945b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_Y_train = pd.get_dummies(train_ds[\"emotions\"], dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a63dc8fa-a27e-4a65-bdcc-f573e9e4eccd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for sentence in train_ds[\"text\"]:\n",
    "    result = tf_idf_class.apply(sentence)\n",
    "    results.append(result)\n",
    "train_ds[\"vectorized\"] = pd.Series(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e64fa85c-c265-4fbf-b82a-555c8fd0c6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 6)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f84a7f0b-1dda-4ef6-853b-4200ac6e4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.asarray(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "47dd4d73-33d6-40a7-8aaa-e36b2baf22de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 1532)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b2bdd8f-a778-46f5-a1ea-228c0eeb0536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tf_idf(data_col, threshold=1):\n",
    "    # Compute tf_idf model, to be used in new sentences.\n",
    "    # word count must be AT LEAST threshold to be accepted\n",
    "    \n",
    "    # Text and split into strings\n",
    "    sentences = data_col.str.lower().str.findall(\"\\w+\")\n",
    "\n",
    "    # All unique words and their counts\n",
    "    word_counter = pd.Series(sentences.sum()).value_counts()\n",
    "\n",
    "    unknown_words = set()\n",
    "    \n",
    "    for word in word_counter.keys():\n",
    "        if word_counter[word] < threshold:\n",
    "            unknown_words.add(word)\n",
    "\n",
    "    print(\"Words with less than \" + str(threshold) + \" occurences: \" + str(unknown_words))\n",
    "    \n",
    "    # Calculate word value counts\n",
    "    word_doc_frequency = dict()\n",
    "    word_doc_frequency[\"_UNKNOWN\"] = 0 # Unknown vector\n",
    "        \n",
    "    for sentence in sentences:\n",
    "        # Get series indexed by word and whose value is the number of occurences\n",
    "        sentence_word_counter = pd.Series(sentence).value_counts()\n",
    "        for word in sentence_word_counter.keys():\n",
    "            if word in unknown_words:\n",
    "                word_doc_frequency[\"_UNKNOWN\"] += sentence_word_counter[word]\n",
    "            else:\n",
    "                if word in word_doc_frequency:\n",
    "                    word_doc_frequency[word] += sentence_word_counter[word]\n",
    "                else:\n",
    "                    word_doc_frequency[word] = sentence_word_counter[word]\n",
    "                \n",
    "    return (len(sentences), word_doc_frequency)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4fdea5d-a303-474e-869e-3e385c0b374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tf_idf(orig_sentence, tf_idf_data):\n",
    "    num_docs, frequencies = tf_idf_data\n",
    "    \n",
    "    # Vocabulary indexing\n",
    "    vocabulary = dict(zip(sorted(frequencies.keys()), range(len(frequencies.keys()))))\n",
    "    \n",
    "    # determine replacement for low-frequency words\n",
    "    sentence = pd.Series(orig_sentence).str.lower().str.findall(\"\\w+\").to_list()[0]\n",
    "    vec = np.zeros((len(frequencies.keys()),))\n",
    "\n",
    "    # Replace with unknowns whenever valid.\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i] not in frequencies.keys():\n",
    "            print(sentence[i] + \" not in \")\n",
    "            sentence[i] = \"_UNKNOWN\"\n",
    "            \n",
    "    # Configure term frequency\n",
    "    term_frequencies = dict(pd.Series(sentence).value_counts()/ len(sentence) )\n",
    "\n",
    "    vectors = np.zeros(len(vocabulary.keys()))\n",
    "    # Implementation of tf_idf\n",
    "    for word in term_frequencies.keys():\n",
    "        tf = term_frequencies[word]\n",
    "        doc_count = frequencies[word] + 1\n",
    "        idf = np.log(num_docs / doc_count)\n",
    "        position = vocabulary[word]\n",
    "        vectors[position] = tf*idf\n",
    "\n",
    "    return vectors, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8efb79eb-ab55-4511-ae5d-20b42c3da5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with less than 1 occurences: set()\n",
      "Topic sentences are similar to mini thesis statements. Like a thesis statement\n",
      "[-0.08173577  0.          0.          0.          0.03378876  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.03378876  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.03378876  0.         -0.02397351  0.03378876  0.          0.\n",
      " -0.14121631 -0.02397351 -0.07060816  0.          0.          0.\n",
      "  0.        ]\n",
      "a topic sentence has a specific main point. Whereas the thesis is the main point of the essay, the topic sentence is the main point of the paragraph. Like the thesis statement, a topic sentence has a unifying function. But a thesis statement or topic sentence alone doesn’t guarantee unity.\n",
      "[-0.09615973  0.          0.0079503   0.          0.          0.0079503\n",
      "  0.0079503   0.          0.0079503   0.0079503   0.          0.\n",
      " -0.02003238  0.         -0.01692247  0.          0.          0.0079503\n",
      "  0.          0.         -0.01692247  0.         -0.05436448  0.\n",
      "  0.          0.0079503  -0.01128165  0.          0.0079503  -0.1902757\n",
      " -0.04984105  0.         -0.06645473  0.          0.0079503   0.0079503\n",
      "  0.        ]\n",
      "An essay is unified if all the paragraphs relate to the thesis, whereas a paragraph is unified if all the sentences relate to the topic sentence.\n",
      "[-0.0377242   0.          0.          0.01559481  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -0.03929428  0.          0.          0.          0.          0.\n",
      "  0.          0.01559481  0.          0.         -0.02665951  0.\n",
      "  0.          0.          0.          0.          0.         -0.21327606\n",
      " -0.03258838 -0.02212939 -0.03258838  0.          0.          0.\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_model = setup_tf_idf(sample_text, threshold=1)\n",
    "results = []\n",
    "for sentence in sample_text.to_list():\n",
    "    print(sentence)\n",
    "    result, vocabulary = apply_tf_idf(sentence, tf_idf_model)\n",
    "    print(result[1:])\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb28e15e-8835-4f0a-b637-7cfd88041dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.        , -0.08173577,  0.        ,  0.        ,  0.        ,\n",
       "         0.03378876,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.03378876,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.03378876,  0.        , -0.02397351,  0.03378876,  0.        ,\n",
       "         0.        , -0.14121631, -0.02397351, -0.07060816,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ]),\n",
       " array([ 0.        , -0.09615973,  0.        ,  0.0079503 ,  0.        ,\n",
       "         0.        ,  0.0079503 ,  0.0079503 ,  0.        ,  0.0079503 ,\n",
       "         0.0079503 ,  0.        ,  0.        , -0.02003238,  0.        ,\n",
       "        -0.01692247,  0.        ,  0.        ,  0.0079503 ,  0.        ,\n",
       "         0.        , -0.01692247,  0.        , -0.05436448,  0.        ,\n",
       "         0.        ,  0.0079503 , -0.01128165,  0.        ,  0.0079503 ,\n",
       "        -0.1902757 , -0.04984105,  0.        , -0.06645473,  0.        ,\n",
       "         0.0079503 ,  0.0079503 ,  0.        ]),\n",
       " array([ 0.        , -0.0377242 ,  0.        ,  0.        ,  0.01559481,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.03929428,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.01559481,  0.        ,  0.        , -0.02665951,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        -0.21327606, -0.03258838, -0.02212939, -0.03258838,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f79c3cdf-445e-469f-834c-5f4a685e6c3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_UNKNOWN': 0,\n",
       " 'a': 1,\n",
       " 'all': 2,\n",
       " 'alone': 3,\n",
       " 'an': 4,\n",
       " 'are': 5,\n",
       " 'but': 6,\n",
       " 'doesn': 7,\n",
       " 'essay': 8,\n",
       " 'function': 9,\n",
       " 'guarantee': 10,\n",
       " 'has': 11,\n",
       " 'if': 12,\n",
       " 'is': 13,\n",
       " 'like': 14,\n",
       " 'main': 15,\n",
       " 'mini': 16,\n",
       " 'of': 17,\n",
       " 'or': 18,\n",
       " 'paragraph': 19,\n",
       " 'paragraphs': 20,\n",
       " 'point': 21,\n",
       " 'relate': 22,\n",
       " 'sentence': 23,\n",
       " 'sentences': 24,\n",
       " 'similar': 25,\n",
       " 'specific': 26,\n",
       " 'statement': 27,\n",
       " 'statements': 28,\n",
       " 't': 29,\n",
       " 'the': 30,\n",
       " 'thesis': 31,\n",
       " 'to': 32,\n",
       " 'topic': 33,\n",
       " 'unified': 34,\n",
       " 'unifying': 35,\n",
       " 'unity': 36,\n",
       " 'whereas': 37}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f183d80-1fc5-4ab8-829a-5fa6288ad6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents:  3\n",
      "Total words:  37\n",
      "{'to': 3, 'unifying': 1, 'unity': 1, 'doesn': 1, 'statement': 3, 'paragraph': 2, 'a': 7, 'if': 2, 'but': 1, 'specific': 1, 'relate': 2, 'has': 2, 'or': 1, 'all': 2, 'sentence': 5, 'similar': 1, 'topic': 6, 'like': 2, 'essay': 2, 'unified': 2, 'are': 1, 'sentences': 2, 'mini': 1, 'main': 3, 't': 1, 'the': 11, 'function': 1, 'is': 4, 'guarantee': 1, 'point': 3, 'an': 1, 'whereas': 2, 'statements': 1, 'paragraphs': 1, 'of': 2, 'thesis': 6, 'alone': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Godonan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "sentences = []\n",
    "word_set = []\n",
    "\n",
    "for sent in sample_text:\n",
    "    words = [word.lower() for word in word_tokenize(sent) if word.isalpha()]\n",
    "    sentences.append(words)\n",
    "    for word in words:\n",
    "        if word not in word_set:\n",
    "            word_set.append(word)# Set of words\n",
    "word_set = set(word_set)\n",
    "# total documents in our corpus\n",
    "total_docs = len(sample_text)\n",
    "print('Total documents: ', total_docs)\n",
    "print('Total words: ', len(word_set))\n",
    "\n",
    "word_index = {}\n",
    "for i, word in enumerate(sorted(word_set)):\n",
    "    word_index[word] = i\n",
    "\n",
    "def count_dict(sentences):\n",
    "    count_dict = {}\n",
    "    for word in word_set:\n",
    "        count_dict[word] = 0\n",
    "    for sent in sentences:\n",
    "        for word in sent:\n",
    "            count_dict[word] += 1\n",
    "    return count_dict\n",
    "    \n",
    "word_count = count_dict(sentences)\n",
    "print(word_count)\n",
    "\n",
    "def term_frequency(document, word):\n",
    "    N = len(document)\n",
    "    occurance = len([token for token in document if token == word])\n",
    "    return occurance / N\n",
    "\n",
    "def inverse_document_frequency(word):\n",
    "    try:\n",
    "        word_occurance = word_count[word] + 1\n",
    "    except:\n",
    "        word_occurance = 1\n",
    "    return np.log(total_docs / word_occurance)\n",
    "\n",
    "def tf_idf(sentence):\n",
    "    vec = np.zeros((len(word_set),))\n",
    "    for word in sentence:\n",
    "        tf = term_frequency(sentence, word)\n",
    "        idf = inverse_document_frequency(word)\n",
    "        vec[word_index[word]] = tf * idf\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ec57fce4-8c60-47d4-82ad-19283727f7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['topic', 'sentences', 'are', 'similar', 'to', 'mini', 'thesis', 'statements', 'like', 'a', 'thesis', 'statement']\n",
      "[-0.08173577  0.          0.          0.          0.03378876  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.03378876  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.03378876  0.         -0.02397351  0.03378876  0.          0.\n",
      " -0.14121631 -0.02397351 -0.07060816  0.          0.          0.\n",
      "  0.        ]\n",
      "['a', 'topic', 'sentence', 'has', 'a', 'specific', 'main', 'point', 'whereas', 'the', 'thesis', 'is', 'the', 'main', 'point', 'of', 'the', 'essay', 'the', 'topic', 'sentence', 'is', 'the', 'main', 'point', 'of', 'the', 'paragraph', 'like', 'the', 'thesis', 'statement', 'a', 'topic', 'sentence', 'has', 'a', 'unifying', 'function', 'but', 'a', 'thesis', 'statement', 'or', 'topic', 'sentence', 'alone', 'doesn', 't', 'guarantee', 'unity']\n",
      "[-0.09615973  0.          0.0079503   0.          0.          0.0079503\n",
      "  0.0079503   0.          0.0079503   0.0079503   0.          0.\n",
      " -0.02003238  0.         -0.01692247  0.          0.          0.0079503\n",
      "  0.          0.         -0.01692247  0.         -0.05436448  0.\n",
      "  0.          0.0079503  -0.01128165  0.          0.0079503  -0.1902757\n",
      " -0.04984105  0.         -0.06645473  0.          0.0079503   0.0079503\n",
      "  0.        ]\n",
      "['an', 'essay', 'is', 'unified', 'if', 'all', 'the', 'paragraphs', 'relate', 'to', 'the', 'thesis', 'whereas', 'a', 'paragraph', 'is', 'unified', 'if', 'all', 'the', 'sentences', 'relate', 'to', 'the', 'topic', 'sentence']\n",
      "[-0.0377242   0.          0.          0.01559481  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -0.03929428  0.          0.          0.          0.          0.\n",
      "  0.          0.01559481  0.          0.         -0.02665951  0.\n",
      "  0.          0.          0.          0.          0.         -0.21327606\n",
      " -0.03258838 -0.02212939 -0.03258838  0.          0.          0.\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print(tf_idf(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61f98912-033c-4c2f-83d9-fc50011bd02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'all': 1,\n",
       " 'alone': 2,\n",
       " 'an': 3,\n",
       " 'are': 4,\n",
       " 'but': 5,\n",
       " 'doesn': 6,\n",
       " 'essay': 7,\n",
       " 'function': 8,\n",
       " 'guarantee': 9,\n",
       " 'has': 10,\n",
       " 'if': 11,\n",
       " 'is': 12,\n",
       " 'like': 13,\n",
       " 'main': 14,\n",
       " 'mini': 15,\n",
       " 'of': 16,\n",
       " 'or': 17,\n",
       " 'paragraph': 18,\n",
       " 'paragraphs': 19,\n",
       " 'point': 20,\n",
       " 'relate': 21,\n",
       " 'sentence': 22,\n",
       " 'sentences': 23,\n",
       " 'similar': 24,\n",
       " 'specific': 25,\n",
       " 'statement': 26,\n",
       " 'statements': 27,\n",
       " 't': 28,\n",
       " 'the': 29,\n",
       " 'thesis': 30,\n",
       " 'to': 31,\n",
       " 'topic': 32,\n",
       " 'unified': 33,\n",
       " 'unifying': 34,\n",
       " 'unity': 35,\n",
       " 'whereas': 36}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68cf415-e930-470d-92cc-2f925dd676ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
